# Distributed locks
在许多环境中，分布式锁是非常有用的，在这些环境中，不同的进程必须以互斥的方式操作共享资源。目前有许多方式实现了分布式锁，官方也提供了一种权威的分布式锁实现“Redlock”。

## Safety and Liveness guarantees（安全与存活的保证）
我们将用三个属性来建模我们的设计，从我们的角度来看，这三个属性是有效使用分布式锁所需的最低保证：
* **Safety property(安全)**：Mutual exclusion(互斥性)，当有多个client时，同一时间只能有一个client获取到锁。
* **Liveness property A**：Deadlock free（无死锁），即使获取到锁的client崩溃了，其他的client也能正常获取到锁。
* **Liveness property B**：Fault tolerance（容错），只要大多数的节点能够正常的运行，客户端就可以获取或释放锁。

## Why failover-based implementations are not enough
基于故障转移的方案不能够保证分布式锁的安全与存活。分析目前大多数基于redis的分布式锁库的现状，发现最简单的方式是去redis的一个实例上创建一个key，用这个key来当共享资源的锁（***接下来的描述中将key等同于锁，请注意，创建key即创建锁***），因为key是可以设置过期时间的，所以这种策略满足无死锁的属性，即使创建key的client崩溃了，key也会自动过期，这样下一个client仍然可以安全的创建key，当然，若client正常运行，当资源处理完成后，client会自动删除该key。表面上这种策略没什么问题，但是这里存在一个隐藏问题，假如这是一个单节点集群，当这个唯一的节点崩溃，怎么办？当然，我们可以加一个slave来做容错，当主节点崩溃，slave充当主节点，但是，由于主从节点的数据同步是异步的，当主节点崩溃的那一刻，仍然未将key同步到从节点，这时，另一个clientB请求改资源，成功创建key，那么clientB与之前的clientA，都成功获取到锁，这时候，Mutual exclusion就无法满足，导致资源出现异常，故增加从节点无法解决因主节点崩溃导致的分布式锁不可用的缺点。

## Correct implementation with a single instance
单节点是实现分布式集群的基础，所以先讨论如何在单节点上正确的实现分布式锁。client去节点上获取锁可以采用以下方式：
`SET resource_name my_random_value NX PX 30000`
【resource_name：资源名称、my_random_value：随机数，全局唯一，保证多个client生成均唯一、NX：目标节点、PX过期时间单位毫秒】
当client能够在节点上成功创建该key，说明client独占了这些资源；当client需要操作完成需要主动删除key，删除时需要判断key是否一致，key一致的情况下判断value是否一致，均满足时，才能够正常删除key，以下为具体实现代码：
`if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end`
为何判断value？
思考：若client设置key的过期时间（即锁有效的时间），小于实际处理资源所花费的时间，将出现，虽然clientA先获取到了锁，但是在处理途中，锁自动过期释放掉了，而clientB在这时去创建key，自然会成功，如果clientA在处理完成后，仅仅通过key一致就删除，由于key是资源的名称，只要操作同一资源，那么key必然是一致的，会导致删除了clientB创建的key，导致clientB的操作出异常。

## The RedLock algorithm
假定存在N个主节点，且每个主节点相互独立，没有任何副本节点，接下来分析如何实现lock的获取。
* 第一步：获取当前的时间毫秒级记录为A1
* 第二步：client试图去N个主节点中创建key，key相同、value随机，client去创建key时，需要设置一个比过期时间小的超时时间（实际上是要小很多），防止在节点上创建key时，由于一些未知异常，等待了非常长的时间，导致已经创建的key有效期到了自动销毁了。比如，可以设置过期时间为10s，超时时间为50毫秒，确保当一个节点不可用时，快速的跳过，访问下一个节点。
* 第三步：client在大于等于N/2+1个节点上成功创建key，即可认为成功获取到了锁。
* 第四步：获取锁之后，实际的锁有效时间=key的过期时间-获取锁过程中消耗的时间。
* 第五步：如果client获取锁失败，将尝试unlock所有节点（即删除已经创建的key）。

**补充思考：**
（1）当获取锁的node崩溃了怎么办，导致实际上锁住的节点<N/2+1，不满足绝大多数？
（2）当锁的时间小于实际操作时间怎么办，相当于实际上有两个进程在操作同一个数据？

## Retry on failure（失败重试机制）
client获取锁失败时，需要隔一段时间重新尝试，这段等待时间可能需要设置的比获取锁消耗的时间略长，这样可以有效避免多个client竞争资源时，发生脑裂，即所有的client都无法达到大多数节点获取锁的条件，导致性能的浪费。此外，为了让client在最短时间内锁住资源，以减少上述情况的发生，在理想情况下client采用multiplexing技术（多路传输），client可以同时访问多个节点，快速获取锁。值得强调的是，对于无法获取大多数锁的client来说，需要尽快释放已经获取的锁，这样就不用等待key到期自动销毁，以便再次获取锁（但是，如果发生网络分区，并且client不能够与Redis节点通信，只能等待key自动过期，会浪费大量性能）。

## Safety arguments
首先，假设client能够在大多数情况下获取锁。所有节点都将包含一个生存时间相同的key。但是，key是在不同的时间设置的，因此key也将在不同的时间到期。但是，如果第一个key在时间T1被设置，而最后一个key在时间t2被设置，则我们确信所有key中，第一个设置的key，将至少存在min_validity=ttl-（t2-t1）-CLOCK_DRIFT（钟漂）。所有其他key都将在稍后过期，因此我们确信这些key将同时设置至少一次。在设置大多数key的过程中，另一个client将无法获取锁，因为如果N/2+1个key已经存在，该client无法再次达到大多数key的条件。因此，如果获取了锁，则不可能同时重新获取它（违反互斥属性）。我们还希望确保同时尝试获取锁的多个client不能同时成功。***如果client锁定的大多数节点使用的时间接近或大于锁的最大有效时间，它将认为锁无效，并将解锁节点***，因此我们只需要考虑client能够锁定大多数实例的时间小于有效期的情况。在这种情况下，对于上面已经表达的参数，为了最小有效性，任何client都不能重新获取锁。因此，只有当锁定大多数节点的时间大于TTL时间时，多个client才能同时锁定n/2+1个节点，使锁定无效。

## Liveness arguments
分布式锁可用性需要以下三个特性保证：
* 自动释放锁（根据过期时间）：最终key可以再次被创建。
* 事实上，大多数情况下，当client没有获取锁或者获取锁后工作正常完成，自动释放锁，不用等待key到过期时间。
* 当client需要重试锁时，可能需要等待一段时间（这段时间可能比获取锁需要花费的时间长），这样可以有效避免clients竞争统一资源时，发生脑裂，导致所有client都一直无法获取到锁。
注意：有时我们在网络分区上支付相当于TTL时间的可用性惩罚，如果有连续分区，我们需要无限期地支付这个惩罚。每次client获取锁并在移除锁之前将其分区时，都会发生这种情况，基本上，如果存在无限的连续网络分区，系统可能在无限长的时间内不可用。

## Performance, crash-recovery and fsync（性能、故障恢复和fsync）
* **Performance**：许多使用redis作为锁服务器的用户需要高性能，包括获取和释放锁的延迟，以及每秒可以执行的获取/释放操作的数量。为了满足这一要求，同时与N个redis服务器对话以减少延迟的策略肯定是多路复用（或者降级的多路复用，即将套接字置于非阻塞模式，发送所有命令，稍后读取所有命令的结果，假设client和每个实例之间的RTT【 RTT是客户到服务器往返所花时间】相似）。
* **crash-recovery and fsync**：如果以崩溃-恢复模型为目标，那么还需要考虑持久性。假设配置Redis时完全没有持久性。client在5个实例中的3个实例中获取锁。client获取锁的其中一个实例被重新启动，此时，可以再次为同一资源锁定3个实例，另一个client可以再次锁定它，这违反了锁的独占性的安全属性。如果启用AOF持久性，事情会有很大改善。例如，可以通过发送关机并重新启动来升级服务器。因为redis expires是语义实现的，所以实际上当服务器关闭时，时间仍然会流逝，所以所有要求都是好的。然而，只要是一个干净的关闭，一切都是好的。停电怎么样？如果redis被配置为每秒在磁盘上进行fsync，那么在重新启动之后，密钥可能会丢失。理论上，如果想要在任何类型的实例重新启动时保证锁的安全性，就需要在持久性设置中启用fsync=always。这反过来又将完全破坏性能，使其达到传统上用于安全实现分布式锁的CP系统的相同级别。不过，事情总比乍一看看上去好。基本上，只要一个实例在崩溃后重新启动，它就不会再参与任何当前活动的锁，所以当实例重新启动时，当前活动的锁集都是通过锁定实例（而不是重新加入系统的实例）获得的。为了保证这一点，只需要让一个崩溃的实例，重新恢复的时间延迟到，当前锁集最大的可用时间之后即可（所有已存在的锁的最大TTL）。使用延迟重启基本上可以实现安全性，即使没有任何可用的Redis持久性，但是请注意，这可能会导致可用性损失。例如，如果大多数实例崩溃，系统将变得对TTL全局不可用（在这里，全局意味着在此期间没有任何资源可锁定）。

## Making the algorithm more reliable: Extending the lock
如果client执行的工作是由小步骤组成的，那么可以默认使用较小的锁有效期，并扩展实现锁扩展机制的算法。基本上，如果在计算过程中，当锁有效性接近一个较低的值时，client可以通过向所有实例发送一个lua脚本来扩展锁，如果该键存在，并且其值仍然是client在获取锁时分配的随机值，则可以扩展该锁。client应该只考虑重新获取的锁，如果它能够将锁扩展到大多数实例中，并且在有效期内（基本上，使用的算法与获取锁时使用的算法非常相似）。但是，这并不能从技术上改变算法，因此应该限制锁重获取尝试的最大次数，否则将违反其中一个活动属性。

